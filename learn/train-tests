#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# This file is part of Cockpit.
#
# Copyright (C) 2017 Slavek Kabrda
#
# Cockpit is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# Cockpit is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Cockpit; If not, see <http://www.gnu.org/licenses/>.

import argparse
import glob
import os
import shutil
import sys
import tempfile
import time
import traceback

sys.dont_write_bytecode = True

import analyze
import cluster
import data
import tempfile

DEFAULT_LIMIT = 15000
DEFAULT_DIRECTORY = "."

class Tee():
    def __init__(self, log, std):
        self.log = log
        self.std = std
    def write(self, *args):
        self.log.write(*args)
        self.log.flush()
        self.std.write(*args)
    def flush(self, *args):
        self.log.flush(*args)
        self.std.flush(*args)
    def close(self, *args):
        self.log.close(*args)
        self.log.close(*args)

def run(filename_or_fp, directory=".", limit=None, verbose=False, name="input", batch=False, **kwargs):
    base = tempfile.mkdtemp(prefix='model-', dir=directory)
    log = open(os.path.join(base, "log"), "w")

    sys.stderr = verbose and Tee(log, sys.__stderr__) or log
    result = 0
    try:
        if verbose:
            sys.stderr.write("{0}\n".format(name))

        # This function tells us which items to keep around for modeling into
        # clusters, however we also do some global analysis as we see each item
        groups = analyze.Groups("statistics")
        def analyze_and_load(item):
            test = item.get("test")
            if test:
                key = (test, item.get("status"), item.get("tracker"), item.get("context"))
                groups.count(key, item, "test")
                groups.count(key, item, "status")
                groups.count(key, item, "tracker")
                groups.count(key, item, "context")
                groups.count(key, item, "merged")
                groups.bound(key, item, "date")
            # In the end only load failures
            return data.failures(item)

        # Atomically place a log at /log
        link = os.path.join(base, "link")
        os.symlink(os.path.join(os.path.basename(base), "log"), link)
        os.rename(link, os.path.join(directory, "log"))

        # Load the data, and pass through above analysis on the way in
        items = data.load(filename_or_fp, only=analyze_and_load, verbose=verbose)

        # Next run the training of the cluster
        train(items, base, limit, verbose)

        # Dump out the test analysis. Note the train actually loads the items
        groups.dump(base)

        # Atomically replace the model into /active
        active = os.path.join(directory, "active")
        try:
            previous = os.readlink(active)
        except IOError:
            previous = None
        link = os.path.join(base, "link")
        os.symlink(os.path.basename(base), link)
        os.rename(link, active)

        # Now clean up any previous active data
        if previous:
            shutil.rmtree(os.path.join(directory, previous))

    # Handle exceptions right here. When in batch mode this
    # really matters, since we want them to also go to the log
    except:
        traceback.print_exc()
        result = 1

    finally:
        sys.stderr.flush()
        sys.stderr = sys.__stderr__
        if batch:
            shutil.move(filename_or_fp, base)

    return result

def train(items, directory, limit, verbose):
    model = cluster.Model(verbose=verbose)
    model.train(items, limit=limit)

    model.dump(directory)
    return cluster.save(directory, model)

def main(function):
    parser = argparse.ArgumentParser(description="Learn from testing data")
    parser.add_argument("-v", "--verbose", action="store_true",
            help="Verbose output")
    parser.add_argument("-l", "--limit", default=os.environ.get("TEST_LIMIT", DEFAULT_LIMIT), type=int,
            help="Number of test failures to learn from")
    parser.add_argument("-d", "--directory", default=os.environ.get("TEST_DATA", DEFAULT_DIRECTORY),
            help="Directory to store cluster data in")
    parser.add_argument("-b", "--batch", action="store_true",
            help="Wait for, process, and delete the input files")
    parser.add_argument("inputs", nargs='+',
            help="Input JSONL GZ file, '-', or glob of files to wait for")
    opts = parser.parse_args()

    task = {
        "verbose": opts.verbose,
        "limit": opts.limit,
        "directory": opts.directory,
        "batch": opts.batch,
    }

    result = 0
    while True:
        for path in opts.inputs:
            if path == "-":
                result = run(sys.stdin.buffer, name="input", **task)
            else:
                for filename in glob.glob(path):
                    result &= run(filename, name=filename, **task)
        if opts.batch:
            time.sleep(5)
        else:
            break

    return result

if __name__ == '__main__':
    sys.exit(main(run))
