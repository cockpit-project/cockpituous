#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# This file is part of Cockpit.
#
# Copyright (C) 2017 Slavek Kabrda
#
# Cockpit is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# Cockpit is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Cockpit; If not, see <http://www.gnu.org/licenses/>.

import argparse
import glob
import os
import shutil
import sys
import tempfile
import time
import traceback
import warnings

sys.dont_write_bytecode = True

import analyze
import cluster
import data

DEFAULT_LIMIT = 15000
DEFAULT_DIRECTORY = "."

class Tee():
    def __init__(self, log, std):
        self.log = log
        self.std = std
    def write(self, *args):
        self.log.write(*args)
        self.log.flush()
        self.std.write(*args)
    def flush(self, *args):
        self.log.flush(*args)
        self.std.flush(*args)
    def close(self, *args):
        self.log.close(*args)
        self.log.close(*args)


def load(filenames, groups, verbose=False):

    # This function tells us which items to keep around for modeling into
    # clusters, however we also do some global analysis as we see each item
    def analyze_and_load(item):
        test = item.get("test")
        if test:
            key = (test, item.get("status"), item.get("tracker"), item.get("context"))
            groups.count(key, item, "test")
            groups.count(key, item, "status")
            groups.count(key, item, "tracker")
            groups.count(key, item, "context")
            groups.count(key, item, "merged")
            groups.bound(key, item, "date")
        # In the end only load failures
        return data.failures(item)

    for filename in filenames:
        if verbose:
            sys.stderr.write("{0}\n".format(os.path.basename(filename)))
        yield from data.load(filename, only=analyze_and_load, verbose=verbose)


def run(filenames, active, directory=".", limit=None, verbose=False, **kwargs):
    base = tempfile.mkdtemp(prefix='model-', dir=directory)
    log = open(os.path.join(base, "log"), "w")

    sys.stderr = verbose and Tee(log, sys.__stderr__) or log
    result = 0
    try:
        # Atomically place a log at /log
        link = os.path.join(base, "link")
        os.symlink(os.path.join(os.path.basename(base), "log"), link)
        os.rename(link, os.path.join(directory, "log"))

        # Load the data, and pass through above analysis on the way in
        groups = analyze.Groups("statistics")
        items = load(filenames, groups, verbose)

        # Next run the training of the cluster
        train(items, base, limit, verbose)

        # Dump out the test analysis. Note the train actually loads the items
        groups.dump(base)

        # Atomically replace the model into /active
        active = os.path.join(directory, "active")
        try:
            previous = os.readlink(active)
        except IOError:
            previous = None
        link = os.path.join(base, "link")
        os.symlink(os.path.basename(base), link)
        os.rename(link, active)

        # Now clean up any previous active data
        if previous:
            shutil.rmtree(os.path.join(directory, previous))

    # Handle exceptions right here. When in batch mode this
    # really matters, since we want them to also go to the log
    except:
        traceback.print_exc()
        result = 1

    finally:
        sys.stderr.flush()
        sys.stderr = sys.__stderr__

    return result


def train(items, directory, limit, verbose):
    model = cluster.Model(verbose=verbose)
    model.train(items, limit=limit)

    model.dump(directory)
    return cluster.save(directory, model)


def main(function):
    parser = argparse.ArgumentParser(description="Learn from testing data")
    parser.add_argument("-v", "--verbose", action="store_true",
            help="Verbose output")
    parser.add_argument("-l", "--limit", default=os.environ.get("TEST_LIMIT", DEFAULT_LIMIT), type=int,
            help="Number of test failures to learn from")
    parser.add_argument("-d", "--directory", default=os.environ.get("TEST_DATA", DEFAULT_DIRECTORY),
            help="Directory to store cluster data in")
    parser.add_argument("-b", "--batch", action="store_true",
            help="Wait for, process, and delete the input files")
    parser.add_argument("inputs", nargs='+',
            help="Input JSONL GZ file, or glob of files to wait for")
    opts = parser.parse_args()

    active = os.path.join(opts.directory, "active")
    task = {
        "verbose": opts.verbose,
        "limit": opts.limit,
        "directory": opts.directory,
        "batch": opts.batch,
        "active": active,
    }

    result = 0

    # The mtime of the stamp file helps us to know if we should process
    try:
        mtime = os.path.getmtime(active)
    except IOError:
        mtime = 0

    while True:
        filenames = []
        changed = not opts.batch

        # Get the list of files to process
        for path in opts.inputs:
            for filename in glob.glob(path):
                if os.path.exists(filename):
                    modified = os.path.getmtime(filename)
                    filenames.append(filename)
                    if modified > mtime:
                        mtime = modified
                        changed = True

        # Now process the files
        if changed:
            result = run(filenames, **task)

        # Place a stamp at /stamp
        if opts.batch:
            time.sleep(5)
        else:
            break

    return result

if __name__ == '__main__':
    sys.exit(main(run))
