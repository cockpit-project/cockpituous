#!/usr/bin/env python

from __future__ import print_function

# Rough configuration file follows, place it in ~/.config/sink

# PruneInterval [days] defaults to 0 (disabled)
DEFAULTS = u"""
[Sink]
Url: http://%(user)s.fedorapeople.org/logs/%(identifier)s/log
Logs: ~/public_html/logs
PruneInterval: 0

[Irc]
Server: chat.freenode.net:6667
Login: %(user)sbot %(user)sbot %(user)sbot
Nick: %(user)sbot
"""

TOKEN = "~/.config/github-token"

import argparse
import errno
import fcntl
import json
import os
import select
import shutil
import socket
import string
import subprocess
import sys
import tempfile
import time
import traceback
import re

# Try Python 3, fail back to Python 2
try:
    from http.client import HTTPSConnection
except ImportError:
    from httplib import HTTPSConnection
try:
    from urllib.parse import urljoin
except ImportError:
    from urlparse import urljoin
try:
    from urllib.request import urlretrieve
except ImportError:
    from urllib import urlretrieve
try:
    from configparser import SafeConfigParser
except ImportError:
    from ConfigParser import SafeConfigParser
try:
    from io import StringIO
except ImportError:
    from cStringIO import StringIO
try:
    MAX_BUFF_SIZE = sys.maxsize
except:
    MAX_BUFF_SIZE = sys.maxint


class GitHubClassic(object):
    def __init__(self, config):
        self.token = None
        try:
            with open(os.path.expanduser(TOKEN), "r") as gt:
                self.token = gt.read().strip()
        except IOError as exc:
            if exc.errno == errno.ENOENT:
                 pass
            else:
                raise

    def push(self, status):
        github = status.get("github", { })
        resource = github.get("resource", None)
        data = github.get("status", None)
        if not self.token or not data or not resource:
            return
        if "description" not in data and "message" in status:
            data["description"] = status["message"]
        if "target_url" not in data:
            data["target_url"] = status["link"]
        headers = {"Content-type": "application/json", "User-Agent": "Cockpit Tests" }
        if self.token:
            headers["Authorization"] = "token " + self.token
        conn = HTTPSConnection("api.github.com", strict=True)
        # conn.set_debuglevel(1)
        conn.request("POST", resource, json.dumps(data), headers)
        response = conn.getresponse()
        output = response.read()
        conn.close()
        if response.status < 200 or response.status >= 300:
            raise RuntimeError("Couldn't update GitHub: {0} {1}\n{2}\n".format(
                               response.status, response.reason, output.strip()))

# Expansion of strings in JSON objects.

def jpath(val, path):
    for p in path:
        if not isinstance(val, dict):
            break
        val = val.get(p, None)
    return val

def expand_str(val, env):
    def sub(m):
        if m.group(1):
            return str(jpath(env, m.group(1).split(".")))
        else:
            return ":"
    return re.sub(r':([-_.a-zA-Z0-9]+)|::', sub, val)

def expand(val, env):
    if isinstance(val, basestring):
        return expand_str(val,env)
    elif isinstance(val, dict):
        return { k: expand(v, env) for k, v in val.items() }
    elif isinstance(val, list):
        return [ expand(v, env) for v in val ]
    else:
        return val

class GitHub(object):
    def __init__(self, config):
        self.token = None
        try:
            with open(os.path.expanduser(TOKEN), "r") as gt:
                self.token = gt.read().strip()
        except IOError as exc:
            if exc.errno == errno.ENOENT:
                 pass
            else:
                raise
        self.results = { }

    def req(self, token, method, resource, data):
        token = token or self.token
        headers = {"Content-type": "application/json", "User-Agent": "Cockpit Tests" }
        if token:
            headers["Authorization"] = "token " + token
        conn = HTTPSConnection("api.github.com", strict=True)
        # conn.set_debuglevel(1)
        conn.request(method, resource, json.dumps(data), headers)
        response = conn.getresponse()
        output = response.read()
        conn.close()
        if response.status < 200 or response.status >= 300:
            raise RuntimeError("Couldn't update GitHub: {0} {1}\n{2}\n".format(
                               response.status, response.reason, output.strip()))
        return json.loads(output)

    def push(self, status):
        github = status.get("github", { })
        token = github.get("token", self.token)
        requests = github.get("requests", None)

        if not token or not requests:
            return

        self.results['link'] = status['link']

        for r in requests:
            method = r.get('method', "GET")
            resource = expand(r.get('resource', None), self.results)
            data = expand(r.get('data', None),  self.results)
            result_name = r.get('result', None)

            if not method or not resource:
                continue

            result = self.req(token, method, resource, data)
            if result_name:
                self.results[result_name] = result

class Irc(object):
    def __init__(self, config):
        (self.server, unused, port) = config.get("Irc", "Server").rpartition(":")
        try:
            self.port = int(port)
        except:
            self.port = 6667
        self.login = config.get("Irc", "Login")
        self.nick = config.get("Irc", "Nick")
        self.pid = 0
        self.pipe = None

    def __del__(self):
        if self.pipe is not None:
            self.pipe.close()
        if self.pid:
            os.waitpid(self.pid, 0)

    def process(self, msgf):
        ircf = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        # Connect to the IRC server
        ircf.connect((self.server, self.port))
        ircf.sendall("USER " + self.login + " :Cockpit tests\n")
        ircf.sendall("NICK " + self.nick + "\n")

        # Our relay buffer and sending logic
        buf = ""
        ready = False
        quit = False

        # Now wait for appropriate events
        inputs = [ msgf, ircf ]
        while inputs:
            (rfds, wfds, efds) = select.select(inputs, [], [])
            for fd in rfds:
                if fd == msgf:
                    data = os.read(msgf, 2048)
                    if not data:
                        inputs.remove(msgf)
                        quit = True
                    buf += data
                elif fd == ircf:
                    data = ircf.recv(2048)
                    if "PING :" in data:
                        ircf.sendall("PONG :pingis\n")
                    if "ERROR :" in data:
                        inputs.remove(ircf)
                    elif self.nick in data:
                        ready = True
            if ready:
                (lines, unused, buf) = buf.rpartition("\n")
                for message in lines.split("\n"):
                    if message:
                        ircf.sendall("PRIVMSG " + message + "\n")
                if quit:
                    ircf.sendall("QUIT\n")

    def start(self):
        if self.pipe is not None:
            return

        [rfd, wfd] = os.pipe()
        sys.stdout.flush()
        sys.stderr.flush()
        self.pid = os.fork()
        if self.pid == 0:
            ret = 0
            os.close(wfd)
            try:
                self.process(rfd)
                sys.stdout.flush()
            except:
                traceback.print_exc()
                ret = 1
            os._exit(ret)
        os.close(rfd)
        self.pipe = os.fdopen(wfd, "w")

    def push(self, status):
        irc = status.get("irc", { })
        message = status.get("message")
        channel = irc.get("channel")
        if message and channel:
            self.start()
            message += " - " + status["link"]
            for line in message.strip().split("\n"):
                self.pipe.write(channel + " :" + line + "\n")
            self.pipe.flush()

def write_new(path, content):
    try:
        os.remove(path)
    except:
        pass
    with open(path, 'wb') as f:
        f.write(content)

class Extras(object):
    def __init__(self, config):
        pass

    def push(self, data):
        if 'extras' in data:
            for url in data['extras']:
                urlretrieve(url, os.path.basename(url))

class Status(object):
    def __init__(self, config, identifier):
        self.reporters = [ GitHubClassic(config), GitHub(config), Irc(config), Extras(config) ]
        self.link = config.get("Sink", "Url").replace('@@', identifier)
        self.data = None

    def push(self, data, log):
        if "link" in data:
            data["link"] = urljoin(self.link, data["link"], True)
        else:
            data["link"] = self.link
        for reporter in self.reporters:
            try:
                reporter.push(data)
            except:
                traceback.print_exc()
                traceback.print_exc(file=log)
        self.data = data

    def begin(self, line, log):
        try:
            data = json.loads(line)
        except:
            sys.stderr.write(self.link + "\n")
            return False
        self.push(data, log)
        sys.stderr.write(data["link"] + "\n")
        return True

    def clean_status(self, data):
        if "github" in data and "token" in data["github"]:
            del data['github']['token']

    def finish(self, line, log):
        if not self.data:
            return False
        used = False
        try:
            data = json.loads(line)
            used = True
        except:
            data = self.data
            if "onaborted" in data:
                data = data["onaborted"]
            else:
                data["message"] = "Aborted"
                if "github" in data and "status" in data["github"]:
                    data["github"]["status"]["state"] = "error"
                    data["github"]["status"]["description"] = "Aborted without status"
        self.push(data, log)
        with open("status", "w") as fp:
            clean_data = data
            self.clean_status(clean_data)
            if "onaborted" in clean_data:
                self.clean_status(clean_data['onaborted'])
            fp.write(json.dumps(clean_data))
        return used

class Buffer(object):

    def __init__(self, fd):
        self.fd = fd
        self.buf = ''

    def push(self, piece):
        self.buf = piece + self.buf

    def readall(self):
        self.read(MAX_BUFF_SIZE)

    def read(self, n=MAX_BUFF_SIZE):
        result = self.buf
        self.buf = ''
        while len(result) < n:
            want = n - len(result)
            data = os.read(self.fd, want > 1024 and 1024 or want)
            if not data:
                break
            result += data
        return result

    def readone(self):
        result = self.buf
        self.buf = ''
        result += os.read(self.fd, 1024)
        return result

# Create the directory in a race free way
def mkdir_and_chdir(base, identifier):
    # make sure the base exists
    if not os.path.exists(base):
        os.makedirs(base)

    directory = os.path.abspath(os.path.join(base, identifier))

    # 1. Create a temporary non-empty directory and get its handle
    tempdir = tempfile.mkdtemp(prefix=identifier, dir=base)
    os.close(os.open(os.path.join(tempdir, ".sink"), os.O_WRONLY | os.O_CREAT))
    dirfd = os.open(tempdir, os.O_RDONLY)

    # 2. Rename directory into place
    while True:
        try:
            os.rename(tempdir, directory)
            break
        except OSError as ex:
            # 3. If that raced, then move the target into our directory
            if ex.errno != errno.EEXIST and ex.errno != errno.ENOTEMPTY:
                raise
            try:
                os.rename(directory, tempfile.mkdtemp(prefix="backup.", dir=tempdir))
            except OSError as ex:
                if ex.errno != errno.ENOENT:
                    raise

    os.fchdir(dirfd)
    os.fchmod(dirfd, 0o755)
    os.close(dirfd)

def prune_logs(base_path=".", dry_run=False):
    """ Check all subdirectories of the current path and delete the ones that
        are either empty or contain only files older than the age threshold.

        For dry runs, print(the name of the directory instead.
    """
    reference_time = time.time()
    age_threshold = 30
    entries = os.listdir(base_path)
    for entry in entries:
        path = os.path.join(base_path, entry)

        # skip non-directories
        if not os.path.isdir(path):
            continue

        # if anything goes wrong, just skip and delete nothing
        # this can happen for instance if some other log process changes dir content while we're pruning
        try:
            # initialize to threshold so empty dirs will be deleted
            last_modified = 0
            # use the last modified time of the logfile itself, not just the directory
            for root, dirs, files in os.walk(path):
                for filename in files:
                    last_modified = max(last_modified, os.stat(os.path.join(root, filename)).st_mtime)

            diff_days = (reference_time - last_modified) / (60*60*24)

            # keep anything newer than 30 days old
            if diff_days < age_threshold:
                continue

            if dry_run:
                sys.stderr.write("Pruning {0} (age: {1} days)\n".format(path, int(round(diff_days))))
            else:
                shutil.rmtree(path)
        except:
            continue

def check_prune(config, logs, dry_run):
    """ See if we have to prune logs, and if we should, fork and do so
        If the main process (log sink) exits before pruning is done,
        the pruning process will be killed. This isn't critical since
        pruning can just resume once the next PruneInterval has elapsed.
    """
    prune_interval = float(config.get("Sink", "PruneInterval"))
    if prune_interval > 0:
        status_file = os.path.join(logs, ".prune")
        prune_now = True
        # use the last modified time of the logfile itself, not just the directory
        if os.path.isfile(status_file):
            reference_time = time.time()
            last_modified = os.stat(status_file).st_mtime
            diff_days = (reference_time - last_modified) / (60*60*24)
            prune_now = diff_days >= prune_interval

        # if we should prune, fork so we don't block the sink
        if prune_now:
            if not os.path.isdir(logs):
                sys.stderr.write("warning: log directory {0} doesn't exist, nothing to do\n".format(logs))
            elif os.fork() == 0:
                # update the file for writing and lock it (prevent concurrent pruning)
                fd = os.open(status_file, os.O_WRONLY | os.O_CREAT | os.O_TRUNC)
                try:
                    fcntl.flock(fd, fcntl.LOCK_NB | fcntl.LOCK_EX)
                except IOError as ex:
                    os.close(fd)
                    print("can't lock, bailing out")
                    sys.stderr.write("Unable to lock log pruning status file: {0}\n".format(status_file))
                    sys.exit(0)

                os.chdir(logs)
                prune_logs(logs, dry_run)
                sys.exit(0)

def main():
    parser = argparse.ArgumentParser(description="Sink logs from distributed processes")
    parser.add_argument('-d', '--dry-run', dest='dry_run', action="store_true", help='Applies only to pruning: do not delete logs')
    parser.add_argument("identifier", nargs=1)
    parser.set_defaults(verbosity=1)
    args = parser.parse_args()

    # Load up configuration if available
    try:
        user = os.environ.get("LOGNAME", os.getlogin())
    except:
        user = os.environ.get("LOGNAME", None)
    config = SafeConfigParser({ "user": user, "identifier": "@@" })
    config.readfp(StringIO(DEFAULTS))
    config.read([ os.path.expanduser("~/.config/sink") ])

    # Create the directory and chdir
    logs = os.path.expanduser(config.get("Sink", "Logs"))

    # See if we have to prune logs
    check_prune(config, logs, args.dry_run)

    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits, )
    identifier = "".join([c if c in valid_chars else '-' for c in args.identifier[0]])
    if not identifier or identifier != args.identifier[0]:
        parser.error("not a valid log identifier: " + identifier)

    mkdir_and_chdir(logs, identifier)

    # Initialize status reporters
    status = Status(config, identifier)

    # Now relay any data until zero byte
    buffer = Buffer(0)
    attached = False
    with open("log", "w") as log:
        count = 0          # Number of lines processed
        last = ""          # Last full output valid line
        done = False       # Set when done
        while not done:
            log.flush()
            sys.stdout.flush()
            data = buffer.readone()
            if not data:
                done = True
            (text, zero, trailing) = (last + data).partition('\x00')
            if len(zero):
                buffer.push(trailing)
                attached = True
                done = True
            lines = text.split("\n")
            last = lines.pop()
            for line in lines:
                line += "\n"
                count += 1
                if count == 1:
                    if status.begin(line, log):
                        continue
                log.write(line)
                sys.stdout.write(line)
        if not status.finish(last, log):
            log.write(last)
            sys.stdout.write(last)

    if attached:
        tar = subprocess.Popen(["tar", "-xzf", "-"], stdin=subprocess.PIPE)
        shutil.copyfileobj(buffer, tar.stdin)
        tar.stdin.close()
        ret = tar.wait()
        if ret:
            raise subprocess.CalledProcessError(ret, "tar")

if __name__ == "__main__":
    main()
