#!/usr/bin/env python3

import argparse
import errno
import fcntl
import json
import os
import pwd
import select
import shutil
import signal
import socket
import string
import subprocess
import sys
import tempfile
import time
import traceback
import re

from http.client import HTTPConnection, HTTPSConnection
from urllib.parse import urljoin, urlparse
from urllib.request import urlretrieve
from configparser import ConfigParser
from io import StringIO

MAX_BUFF_SIZE = sys.maxsize
EXIT_COLLISION = 77


# Rough configuration file follows, place it in ~/.config/sink

# PruneInterval [days] defaults to 0 (disabled)
DEFAULTS = u"""
[Sink]
Url: http://%(user)s.fedorapeople.org/logs/%(identifier)s/log
Logs: ~/public_html/logs
PruneInterval: 0

[Irc]
Server: chat.freenode.net:6667
Login: %(user)sbot %(user)sbot %(user)sbot
Nick: %(user)sbot

[GitHub]
Server: https://api.github.com
"""

TOKEN = "~/.config/github-token"

# binary streams; in main() they get tee'd to log file
stdout = sys.stdout.buffer
stderr = sys.stderr.buffer


# Expansion of strings in JSON objects.

def jpath(val, path):
    for p in path:
        if not isinstance(val, dict):
            break
        val = val.get(p, None)
    return val


def expand_str(val, env):
    def sub(m):
        if m.group(1):
            return str(jpath(env, m.group(1).split(".")))
        else:
            return ":"
    return re.sub(r':([-_.a-zA-Z0-9]+)|::', sub, val)


def expand(val, env):
    if isinstance(val, str):
        return expand_str(val, env)
    elif isinstance(val, dict):
        return {k: expand(v, env) for k, v in val.items()}
    elif isinstance(val, list):
        return [expand(v, env) for v in val]
    else:
        return val


# Compare two JSON style values to see if one is contained in the other
# For lists and dict make sure all items in expect are also present in result
# For any other values a strict comparison is used
def contains(expect, result, debug=True):
    if isinstance(expect, list) and isinstance(result, list):
        for exp in expect:
            found = 0
            for res in result:
                if contains(exp, res, False):
                    found += 1
            if found == 0:
                if debug:
                    stderr.write(("Expected %s but not present in %s\n" % (
                        json.dumps(exp), json.dumps(result))).encode())
                return False
            elif found > 1:
                if debug:
                    stderr.write(("Expected one %s but found %s times\n" % (
                        json.dumps(exp), str(found))).encode())
                return False
        return True
    elif isinstance(expect, dict) and isinstance(result, dict):
        for key in expect:
            if not contains(expect[key], result.get(key), debug):
                return False
        return True
    elif expect != result:
        if debug:
            stderr.write(("Expecting %s but saw %s\n" % (json.dumps(expect), json.dumps(result))).encode())
        return False
    else:
        return True


def github_usr1_handler(sig, frame):
    stderr.write(b"State not as expected. Possible collision. Aborting.\n")
    stderr.flush()
    sys.exit(EXIT_COLLISION)


class GitHub(object):
    def __init__(self, config):
        self.token = None
        self.url = urlparse(config.get("GitHub", "Server"))
        try:
            with open(os.path.expanduser(TOKEN), "r") as gt:
                self.token = gt.read().strip()
        except IOError as exc:
            if exc.errno == errno.ENOENT:
                pass
            else:
                raise
        self.results = {}
        self.child = 0
        signal.signal(signal.SIGUSR1, github_usr1_handler)

    def __del__(self):
        self.cleanup()

    def req(self, token, request):
        method = request.get('method', "GET")
        resource = expand(request.get('resource', None), self.results)
        data = expand(request.get('data', None), self.results)
        token = request.get('token', token or self.token)
        headers = {"Content-type": "application/json", "User-Agent": "Cockpit Tests"}
        if token:
            headers["Authorization"] = "token " + token
        if self.url.scheme == 'http':
            conn = HTTPConnection(self.url.netloc)
        else:
            conn = HTTPSConnection(self.url.netloc)
        # conn.set_debuglevel(1)
        conn.request(method, resource, data and json.dumps(data) or None, headers)
        response = conn.getresponse()
        output = response.read()
        conn.close()
        if response.status < 200 or response.status >= 300:
            raise RuntimeError("Couldn't update GitHub: {0} {1}\n{2}\n".format(
                               response.status, response.reason, output.strip()))
        return json.loads(output.decode())

    def cleanup(self):
        child = self.child
        self.child = 0
        if child == 0:
            return
        try:
            os.kill(child, signal.SIGTERM)
            os.waitpid(child, 0)
        except OSError:
            pass

    # Check if a given field in GitHub is not as expected
    def watches(self, token, watches):
        while len(watches):
            for watch in list(watches):
                interval = watch.get('interval', 0)
                time.sleep(interval or 60)
                if not interval:
                    watches.remove(watch)
                result = self.req(token, watch)
                expected = expand(watch.get('result', None), self.results)
                if expected is not None and not contains(expected, result):
                    return False
        return True

    def push(self, status):
        github = status.get("github", {})
        http = status.get("http", {})
        token = github.get("token")
        requests = (github.get("requests") or []) + (http.get("requests") or [])

        for r in requests:
            self.results['link'] = status['link']
            result_name = r.get('result', None)

            result = self.req(token, r)
            if result_name:
                self.results[result_name] = result

        # Cleanup any previous watches
        self.cleanup()

        # Expect that a value is present on GitHub
        watches = (github.get("watches") or []) + (http.get("watches") or [])
        if watches:
            self.child = os.fork()
            if self.child == 0:
                ret = 0
                try:
                    if not self.watches(token, watches):
                        # signal to the parent process to exit with code 77
                        os.kill(os.getppid(), signal.SIGUSR1)
                        ret = 1
                except:
                    traceback.print_exc()
                os._exit(ret)


class Irc(object):
    def __init__(self, config):
        (self.server, unused, port) = config.get("Irc", "Server").rpartition(":")
        try:
            self.port = int(port)
        except:
            self.port = 6667
        self.login = config.get("Irc", "Login")
        self.nick = config.get("Irc", "Nick")
        self.pid = 0
        self.pipe = None

    def __del__(self):
        if self.pipe is not None:
            self.pipe.close()
        if self.pid:
            os.waitpid(self.pid, 0)

    def process(self, msgf):
        ircf = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        # Connect to the IRC server
        ircf.connect((self.server, self.port))
        ircf.sendall("USER " + self.login + " :Cockpit tests\n")
        ircf.sendall("NICK " + self.nick + "\n")

        # Our relay buffer and sending logic
        buf = b''
        ready = False
        quit = False

        # Now wait for appropriate events
        inputs = [msgf, ircf]
        while inputs:
            (rfds, wfds, efds) = select.select(inputs, [], [])
            for fd in rfds:
                if fd == msgf:
                    data = os.read(msgf, 2048)
                    if not data:
                        inputs.remove(msgf)
                        quit = True
                    buf += data
                elif fd == ircf:
                    data = ircf.recv(2048)
                    if "PING :" in data:
                        ircf.sendall("PONG :pingis\n")
                    if "ERROR :" in data:
                        inputs.remove(ircf)
                    elif self.nick in data:
                        ready = True
            if ready:
                (lines, unused, buf) = buf.rpartition(b"\n")
                for message in lines.split(b"\n"):
                    if message:
                        ircf.sendall("PRIVMSG " + message + "\n")
                if quit:
                    ircf.sendall("QUIT\n")

    def start(self):
        if self.pipe is not None:
            return

        [rfd, wfd] = os.pipe()
        stdout.flush()
        stderr.flush()
        self.pid = os.fork()
        if self.pid == 0:
            ret = 0
            os.close(wfd)
            try:
                self.process(rfd)
                stdout.flush()
            except:
                traceback.print_exc()
                ret = 1
            os._exit(ret)
        os.close(rfd)
        self.pipe = os.fdopen(wfd, "w")

    def push(self, status):
        irc = status.get("irc", {})
        message = status.get("message")
        channel = irc.get("channel")
        if message and channel:
            self.start()
            message += " - " + status["link"]
            for line in message.strip().split("\n"):
                self.pipe.write(channel + " :" + line + "\n")
            self.pipe.flush()


def write_new(path, content):
    try:
        os.remove(path)
    except:
        pass
    with open(path, 'wb') as f:
        f.write(content)


class Extras(object):
    def __init__(self, config):
        pass

    def push(self, data):
        if 'extras' in data:
            for url in data['extras']:
                urlretrieve(url, os.path.basename(url))


class Status(object):
    def __init__(self, config, identifier):
        self.reporters = [GitHub(config), Irc(config), Extras(config)]
        self.link = config.get("Sink", "Url").replace('@@', identifier)
        self.data = None

    def push(self, data, log):
        if "link" in data:
            data["link"] = urljoin(self.link, data["link"], True)
        else:
            data["link"] = self.link
        for reporter in self.reporters:
            try:
                reporter.push(data)
            except Exception:
                traceback.print_exc()
                log.write(traceback.format_exc().encode('UTF-8'))
        self.data = data

    def begin(self, line, log):
        try:
            data = json.loads(line.decode())
        except:
            stderr.write((self.link + "\n").encode())
            return False
        self.push(data, log)
        stderr.write((data["link"] + "\n").encode())
        return True

    def clean_status(self, data):
        if "github" in data and "token" in data["github"]:
            del data['github']['token']

    def finish(self, line, log):
        if not self.data:
            return False
        used = False
        try:
            data = json.loads(line.decode())
            used = True
        except:
            data = self.data
            if "onaborted" in data:
                data = data["onaborted"]
            else:
                data["message"] = "Aborted"
                if "github" in data and "status" in data["github"]:
                    data["github"]["status"]["state"] = "error"
                    data["github"]["status"]["description"] = "Aborted without status"
        self.push(data, log)
        with open("status", "w") as fp:
            clean_data = data
            self.clean_status(clean_data)
            if "onaborted" in clean_data:
                self.clean_status(clean_data['onaborted'])
            fp.write(json.dumps(clean_data))
        return used


class Buffer(object):

    def __init__(self, fd):
        self.fd = fd
        self.buf = b''

    def push(self, piece):
        self.buf = piece + self.buf

    def readall(self):
        self.read(MAX_BUFF_SIZE)

    def read(self, n=MAX_BUFF_SIZE):
        result = self.buf
        self.buf = b''
        while len(result) < n:
            want = n - len(result)
            data = os.read(self.fd, want > 1024 and 1024 or want)
            if not data:
                break
            result += data
        return result

    def readone(self):
        result = self.buf
        self.buf = b''
        result += os.read(self.fd, 1024)
        return result


class Tee():
    def __init__(self, log, std):
        self.log = log
        self.std = std

    def write(self, *args):
        self.log.write(*args)
        self.std.write(*args)

    def flush(self, *args):
        self.log.flush(*args)
        self.std.flush(*args)

    def close(self, *args):
        self.log.close(*args)
        self.log.close(*args)


# Create the directory in a race free way
def mkdir_and_chdir(base, identifier):
    # make sure the base exists
    if not os.path.exists(base):
        os.makedirs(base)

    directory = os.path.abspath(os.path.join(base, identifier))

    # 1. Create a temporary non-empty directory and get its handle
    tempdir = tempfile.mkdtemp(prefix=identifier, dir=base)
    os.close(os.open(os.path.join(tempdir, ".sink"), os.O_WRONLY | os.O_CREAT))
    dirfd = os.open(tempdir, os.O_RDONLY)

    # 2. Rename directory into place
    while True:
        try:
            os.rename(tempdir, directory)
            break
        except OSError as ex:
            # 3. If that raced, then move the target into our directory
            if ex.errno != errno.EEXIST and ex.errno != errno.ENOTEMPTY:
                raise
            try:
                os.rename(directory, tempfile.mkdtemp(prefix="backup.", dir=tempdir))
            except OSError as ex:
                if ex.errno != errno.ENOENT:
                    raise

    os.fchdir(dirfd)
    os.fchmod(dirfd, 0o755)
    os.close(dirfd)


def prune_logs(base_path=".", dry_run=False):
    """ Check all subdirectories of the current path and delete the ones that
        are either empty or contain only files older than the age threshold.

        For dry runs, print(the name of the directory instead.
    """
    reference_time = time.time()
    age_threshold = 30
    entries = os.listdir(base_path)
    for entry in entries:
        path = os.path.join(base_path, entry)

        # skip non-directories
        if not os.path.isdir(path):
            continue

        # if anything goes wrong, just skip and delete nothing
        # this can happen for instance if some other log process changes dir content while we're pruning
        try:
            # initialize to threshold so empty dirs will be deleted
            last_modified = 0
            # use the last modified time of the logfile itself, not just the directory
            for root, dirs, files in os.walk(path):
                for filename in files:
                    last_modified = max(last_modified, os.stat(os.path.join(root, filename)).st_mtime)

            diff_days = (reference_time - last_modified) / (60 * 60 * 24)

            # keep anything newer than 30 days old
            if diff_days < age_threshold:
                continue

            if dry_run:
                stderr.write(("Pruning {0} (age: {1} days)\n".format(path, int(round(diff_days)))).encode())
            else:
                shutil.rmtree(path)
        except:
            continue


def check_prune(config, logs, dry_run):
    """ See if we have to prune logs, and if we should, fork and do so
        If the main process (log sink) exits before pruning is done,
        the pruning process will be killed. This isn't critical since
        pruning can just resume once the next PruneInterval has elapsed.
    """
    prune_interval = float(config.get("Sink", "PruneInterval"))
    if prune_interval > 0:
        status_file = os.path.join(logs, ".prune")
        prune_now = True
        # use the last modified time of the logfile itself, not just the directory
        if os.path.isfile(status_file):
            reference_time = time.time()
            last_modified = os.stat(status_file).st_mtime
            diff_days = (reference_time - last_modified) / (60 * 60 * 24)
            prune_now = diff_days >= prune_interval

        # if we should prune, fork so we don't block the sink
        if prune_now:
            if not os.path.isdir(logs):
                stderr.write(("warning: log directory {0} doesn't exist, nothing to do\n".format(logs)).encode())
            elif os.fork() == 0:
                # update the file for writing and lock it (prevent concurrent pruning)
                fd = os.open(status_file, os.O_WRONLY | os.O_CREAT | os.O_TRUNC)
                try:
                    fcntl.flock(fd, fcntl.LOCK_NB | fcntl.LOCK_EX)
                except IOError:
                    os.close(fd)
                    print("can't lock, bailing out")
                    stderr.write(("Unable to lock log pruning status file: {0}\n".format(status_file)).encode())
                    sys.exit(0)

                os.chdir(logs)
                prune_logs(logs, dry_run)
                sys.exit(0)


def main():
    parser = argparse.ArgumentParser(description="Sink logs from distributed processes")
    parser.add_argument('-d', '--dry-run', dest='dry_run', action="store_true",
                        help='Applies only to pruning: do not delete logs')
    parser.add_argument('-c', '--config', dest='config', default="~/.config/sink",
                        help='Set an alternate config file')
    parser.add_argument("identifier", nargs=1)
    parser.set_defaults(verbosity=1)
    args = parser.parse_args()

    # Load up configuration if available
    user = os.environ.get("LOGNAME")
    if not user:
        try:
            user = os.getlogin()
        except OSError:
            user = pwd.getpwuid(os.getuid()).pw_name
    config = ConfigParser({"user": user, "identifier": "@@"})
    config.read_file(StringIO(DEFAULTS))
    config.read([os.path.expanduser(args.config)])

    # Create the directory and chdir
    logs = os.path.expanduser(config.get("Sink", "Logs"))

    # See if we have to prune logs
    check_prune(config, logs, args.dry_run)

    valid_chars = "-_.%s%s" % (string.ascii_letters, string.digits, )
    identifier = "".join([c if c in valid_chars else '-' for c in args.identifier[0]])
    if not identifier or identifier != args.identifier[0]:
        parser.error("not a valid log identifier: " + identifier)

    mkdir_and_chdir(logs, identifier)

    # Initialize status reporters
    status = Status(config, identifier)

    log = open("log", "wb")
    # treat streams as binary stream, to avoid problems with non-ASCII data
    global stdout, stderr
    stdout = Tee(log, sys.stdout.buffer)
    stderr = Tee(log, sys.stderr.buffer)

    # Now relay any data until zero byte
    buffer = Buffer(0)
    attached = False
    count = 0          # Number of lines processed
    last = b""         # Last full output valid line
    done = False       # Set when done
    while not done:
        stdout.flush()
        data = buffer.readone()
        if not data:
            done = True
        (text, zero, trailing) = (last + data).partition(b'\x00')
        if len(zero):
            buffer.push(trailing)
            attached = True
            done = True
        lines = text.split(b"\n")
        last = lines.pop()
        for line in lines:
            line += b"\n"
            count += 1
            if count == 1:
                if status.begin(line, log):
                    continue
            stdout.write(line)
    if not status.finish(last, log):
        stdout.write(last)

    if attached:
        tar = subprocess.Popen(["tar", "-xzf", "-"], stdin=subprocess.PIPE)
        shutil.copyfileobj(buffer, tar.stdin)
        tar.stdin.close()
        ret = tar.wait()
        if ret:
            raise subprocess.CalledProcessError(ret, "tar")
        subprocess.check_call(["chmod", "--recursive", "ugo+r", "."])


if __name__ == "__main__":
    main()
